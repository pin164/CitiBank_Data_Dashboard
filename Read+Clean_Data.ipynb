{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read '202309-citibike-tripdata_1.csv'\n",
      "Successfully cleaned and sampled '202309-citibike-tripdata_1.csv'\n",
      "Successfully read '202310-citibike-tripdata_1.csv'\n",
      "Successfully cleaned and sampled '202310-citibike-tripdata_1.csv'\n",
      "Successfully read '202311-citibike-tripdata_1.csv'\n",
      "Successfully cleaned and sampled '202311-citibike-tripdata_1.csv'\n",
      "Successfully read '202312-citibike-tripdata_1.csv'\n",
      "Successfully cleaned and sampled '202312-citibike-tripdata_1.csv'\n",
      "Successfully read '202401-citibike-tripdata.csv'\n",
      "Successfully cleaned and sampled '202401-citibike-tripdata.csv'\n",
      "Successfully read '202402-citibike-tripdata.csv'\n",
      "Successfully cleaned and sampled '202402-citibike-tripdata.csv'\n",
      "Successfully read '202403-citibike-tripdata.csv'\n",
      "Successfully cleaned and sampled '202403-citibike-tripdata.csv'\n",
      "Successfully read '202404-citibike-tripdata.csv'\n",
      "Successfully cleaned and sampled '202404-citibike-tripdata.csv'\n",
      "Successfully read '202405-citibike-tripdata_1.csv'\n",
      "Successfully cleaned and sampled '202405-citibike-tripdata_1.csv'\n",
      "Successfully read '202406-citibike-tripdata_1.csv'\n",
      "Successfully cleaned and sampled '202406-citibike-tripdata_1.csv'\n",
      "Successfully read '202407-citibike-tripdata_1.csv'\n",
      "Successfully cleaned and sampled '202407-citibike-tripdata_1.csv'\n",
      "Successfully read '202408-citibike-tripdata_1.csv'\n",
      "Successfully cleaned and sampled '202408-citibike-tripdata_1.csv'\n",
      "\n",
      "All cleaned DataFrames have been successfully combined into 'tripdata_df'.\n",
      "\n",
      "Sample of the combined DataFrame:\n",
      "            ride_id  rideable_type              started_at  \\\n",
      "0  EFC4029DB4ADE23E   classic_bike 2023-09-05 13:08:38.032   \n",
      "1  FB1DE2CA38C44BBE  electric_bike 2023-09-30 14:42:24.920   \n",
      "2  B3AD043520D8437D   classic_bike 2023-09-05 08:03:53.556   \n",
      "3  69627B5D5A102FB8   classic_bike 2023-09-30 14:48:52.633   \n",
      "4  FEA22E9FB9D641B0   classic_bike 2023-09-20 14:27:41.509   \n",
      "\n",
      "                 ended_at                   start_station_name  \\\n",
      "0 2023-09-05 13:23:05.790                 31 Ave & Crescent St   \n",
      "1 2023-09-30 14:44:12.437       Metropolitan Ave & Bedford Ave   \n",
      "2 2023-09-05 08:29:05.885  Adam Clayton Powell Blvd & W 141 St   \n",
      "3 2023-09-30 14:58:15.976                Spring St & Hudson St   \n",
      "4 2023-09-20 14:33:14.101               Irving Ave & Halsey St   \n",
      "\n",
      "  start_station_id                end_station_name end_station_id  start_lat  \\\n",
      "0          6893.10             36 St & Queens Blvd        6260.04  40.765835   \n",
      "1          5308.04  Metropolitan Ave & Bedford Ave        5308.04  40.715340   \n",
      "2          7893.05                12 Ave & W 40 St        6765.01  40.819241   \n",
      "3          5653.12             Bank St & Hudson St        5922.08  40.725840   \n",
      "4          4695.04         Wyckoff Ave & Gates Ave        4847.03  40.694670   \n",
      "\n",
      "   start_lng    end_lat    end_lng member_casual  Unnamed: 0  \\\n",
      "0 -73.926547  40.744080 -73.929010        casual         NaN   \n",
      "1 -73.960250  40.715348 -73.960241        member         NaN   \n",
      "2 -73.941057  40.760875 -74.002777        member         NaN   \n",
      "3 -74.007653  40.736529 -74.006180        casual         NaN   \n",
      "4 -73.906630  40.699871 -73.911719        member         NaN   \n",
      "\n",
      "  rideable_type_duplicate_column_name_1  \n",
      "0                                   NaN  \n",
      "1                                   NaN  \n",
      "2                                   NaN  \n",
      "3                                   NaN  \n",
      "4                                   NaN  \n",
      "\n",
      "Total records in 'tripdata_df': 6000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                 Non-Null Count  Dtype         \n",
      "---  ------                                 --------------  -----         \n",
      " 0   ride_id                                6000 non-null   object        \n",
      " 1   rideable_type                          6000 non-null   object        \n",
      " 2   started_at                             6000 non-null   datetime64[ns]\n",
      " 3   ended_at                               6000 non-null   datetime64[ns]\n",
      " 4   start_station_name                     6000 non-null   object        \n",
      " 5   start_station_id                       6000 non-null   object        \n",
      " 6   end_station_name                       6000 non-null   object        \n",
      " 7   end_station_id                         6000 non-null   object        \n",
      " 8   start_lat                              6000 non-null   float64       \n",
      " 9   start_lng                              6000 non-null   float64       \n",
      " 10  end_lat                                6000 non-null   float64       \n",
      " 11  end_lng                                6000 non-null   float64       \n",
      " 12  member_casual                          6000 non-null   object        \n",
      " 13  Unnamed: 0                             500 non-null    float64       \n",
      " 14  rideable_type_duplicate_column_name_1  500 non-null    object        \n",
      "dtypes: datetime64[ns](2), float64(5), object(8)\n",
      "memory usage: 703.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path to the 'Resources' folder\n",
    "# Adjust the path as necessary based on your project's structure\n",
    "resources_path = Path(\"Resources\")\n",
    "\n",
    "# Check if the 'Resources' folder exists\n",
    "if not resources_path.exists():\n",
    "    # If the folder does not exist, print an error message\n",
    "    print(f\"Error: The folder '{resources_path}' does not exist.\")\n",
    "else:\n",
    "    # Use glob to find all CSV files in the 'Resources' folder\n",
    "    # The pattern '*.csv' matches all files with a .csv extension\n",
    "    csv_files = list(resources_path.glob(\"*.csv\"))\n",
    "    \n",
    "    # Check if any CSV files were found\n",
    "    if not csv_files:\n",
    "        # If no CSV files are found, print an error message\n",
    "        print(\"Error: No CSV files found in the 'Resources' folder.\")\n",
    "    else:\n",
    "        # Initialize an empty list to store cleaned DataFrames\n",
    "        # This will be used to concatenate all cleaned DataFrames later\n",
    "        cleaned_dataframes = []\n",
    "        \n",
    "        # Iterate over each CSV file found\n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                # Read the CSV file into a DataFrame using pandas\n",
    "                # low_memory=False helps in efficiently reading large files by processing them in chunks\n",
    "                df = pd.read_csv(csv_file, low_memory=False)\n",
    "                \n",
    "                # Print a success message indicating the file was read\n",
    "                print(f\"Successfully read '{csv_file.name}'\")\n",
    "                \n",
    "                # -------------------- Data Cleaning Steps --------------------\n",
    "                \n",
    "                # 1. Replace empty strings or strings with only whitespace with NaN\n",
    "                # This standardizes missing or undefined data for easier processing\n",
    "                df.replace(r'^\\s*$', pd.NA, regex=True, inplace=True)\n",
    "                \n",
    "                # 2. Convert 'started_at' and 'ended_at' columns to datetime format\n",
    "                # This ensures that date and time operations can be performed on these columns\n",
    "                # Errors='coerce' will convert invalid parsing to NaT (Not a Time)\n",
    "                df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "                df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "                \n",
    "                # 3. Drop rows with any missing values, including NaN and NaT\n",
    "                # This ensures that the dataset only contains complete records\n",
    "                df.dropna(inplace=True)\n",
    "                \n",
    "                # 4. Sample 500 rows if there are at least 500; otherwise, use all rows\n",
    "                # This helps in managing memory usage and ensures consistency in dataset size\n",
    "                if len(df) >= 500:\n",
    "                    df_sampled = df.sample(n=500, random_state=42)  # random_state for reproducibility\n",
    "                else:\n",
    "                    df_sampled = df.copy()  # Use all rows if less than 500\n",
    "                \n",
    "                # 5. Append the cleaned and sampled DataFrame to the list\n",
    "                cleaned_dataframes.append(df_sampled)\n",
    "                \n",
    "                # Print a message indicating that cleaning was successful for this file\n",
    "                print(f\"Successfully cleaned and sampled '{csv_file.name}'\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                # If an error occurs while reading or processing the file, print an error message\n",
    "                print(f\"Error processing '{csv_file.name}': {e}\")\n",
    "        \n",
    "        # After processing all files, check if there are any cleaned DataFrames to concatenate\n",
    "        if cleaned_dataframes:\n",
    "            try:\n",
    "                # Concatenate all cleaned DataFrames into a single DataFrame\n",
    "                # ignore_index=True resets the index in the resulting DataFrame\n",
    "                tripdata_df = pd.concat(cleaned_dataframes, ignore_index=True)\n",
    "                \n",
    "                # Print a success message indicating the DataFrames were concatenated\n",
    "                print(\"\\nAll cleaned DataFrames have been successfully combined into 'tripdata_df'.\")\n",
    "                \n",
    "                # Optionally, display the first few rows of the combined DataFrame\n",
    "                print(\"\\nSample of the combined DataFrame:\")\n",
    "                print(tripdata_df.head())\n",
    "                \n",
    "                # Optionally, display the shape of the combined DataFrame\n",
    "                print(f\"\\nTotal records in 'tripdata_df': {tripdata_df.shape[0]}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                # If an error occurs during concatenation, print an error message\n",
    "                print(f\"Error concatenating DataFrames: {e}\")\n",
    "        else:\n",
    "            # If no DataFrames were cleaned and collected, inform the user\n",
    "            print(\"No DataFrames were cleaned and collected. 'tripdata_df' was not created.\")\n",
    "# Display the tripdate\n",
    "tripdata_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                 Non-Null Count  Dtype         \n",
      "---  ------                                 --------------  -----         \n",
      " 0   ride_id                                6000 non-null   object        \n",
      " 1   rideable_type                          6000 non-null   object        \n",
      " 2   started_at                             6000 non-null   datetime64[ns]\n",
      " 3   ended_at                               6000 non-null   datetime64[ns]\n",
      " 4   start_station_name                     6000 non-null   object        \n",
      " 5   start_station_id                       6000 non-null   object        \n",
      " 6   end_station_name                       6000 non-null   object        \n",
      " 7   end_station_id                         6000 non-null   object        \n",
      " 8   start_lat                              6000 non-null   float64       \n",
      " 9   start_lng                              6000 non-null   float64       \n",
      " 10  end_lat                                6000 non-null   float64       \n",
      " 11  end_lng                                6000 non-null   float64       \n",
      " 12  member_casual                          6000 non-null   object        \n",
      " 13  Unnamed: 0                             500 non-null    float64       \n",
      " 14  rideable_type_duplicate_column_name_1  500 non-null    object        \n",
      "dtypes: datetime64[ns](2), float64(5), object(8)\n",
      "memory usage: 703.2+ KB\n"
     ]
    }
   ],
   "source": [
    "tripdata_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully separated 'started_at' and 'ended_at' into date (datetime64[ns]) and time columns.\n",
      "\n",
      "Sample of 'tripdata_df' with new date and time columns:\n",
      "               started_at started_at_date started_at_time  \\\n",
      "0 2023-09-05 13:08:38.032      2023-09-05        13:08:38   \n",
      "1 2023-09-30 14:42:24.920      2023-09-30        14:42:24   \n",
      "2 2023-09-05 08:03:53.556      2023-09-05        08:03:53   \n",
      "3 2023-09-30 14:48:52.633      2023-09-30        14:48:52   \n",
      "4 2023-09-20 14:27:41.509      2023-09-20        14:27:41   \n",
      "\n",
      "                 ended_at ended_at_date ended_at_time  \n",
      "0 2023-09-05 13:23:05.790    2023-09-05      13:23:05  \n",
      "1 2023-09-30 14:44:12.437    2023-09-30      14:44:12  \n",
      "2 2023-09-05 08:29:05.885    2023-09-05      08:29:05  \n",
      "3 2023-09-30 14:58:15.976    2023-09-30      14:58:15  \n",
      "4 2023-09-20 14:33:14.101    2023-09-20      14:33:14  \n",
      "\n",
      "Data types of the new date and time columns:\n",
      "started_at_date    datetime64[ns]\n",
      "started_at_time            object\n",
      "ended_at_date      datetime64[ns]\n",
      "ended_at_time              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Separate 'started_at' and 'ended_at' into Date and Time Components --------------------\n",
    "\n",
    "# 1. Create a new column 'started_at_date' by extracting only the date part and converting to datetime64[ns]\n",
    "# We use .dt.floor('D') to get just the date part and ensure it's datetime64[ns]\n",
    "tripdata_df['started_at_date'] = tripdata_df['started_at'].dt.floor('D')\n",
    "\n",
    "# 2. Create a new column 'started_at_time' by extracting the time part\n",
    "# Pandas does not support a datetime64[ns] type for time only, so we leave it as an object or string\n",
    "tripdata_df['started_at_time'] = tripdata_df['started_at'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "# 3. Create a new column 'ended_at_date' by extracting only the date part and converting to datetime64[ns]\n",
    "tripdata_df['ended_at_date'] = tripdata_df['ended_at'].dt.floor('D')\n",
    "\n",
    "# 4. Create a new column 'ended_at_time' by extracting the time part\n",
    "tripdata_df['ended_at_time'] = tripdata_df['ended_at'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "# -------------------- Optional: Verify the New Columns --------------------\n",
    "\n",
    "# Print a confirmation message to indicate that the separation was successful\n",
    "print(\"\\nSuccessfully separated 'started_at' and 'ended_at' into date (datetime64[ns]) and time columns.\")\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the new columns\n",
    "print(\"\\nSample of 'tripdata_df' with new date and time columns:\")\n",
    "print(tripdata_df[['started_at', 'started_at_date', 'started_at_time', \n",
    "                   'ended_at', 'ended_at_date', 'ended_at_time']].head())\n",
    "\n",
    "# Optionally, display the data types of the new columns to confirm their types\n",
    "print(\"\\nData types of the new date and time columns:\")\n",
    "print(tripdata_df[['started_at_date', 'started_at_time', \n",
    "                   'ended_at_date', 'ended_at_time']].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                                 Non-Null Count  Dtype         \n",
      "---  ------                                 --------------  -----         \n",
      " 0   ride_id                                6000 non-null   object        \n",
      " 1   rideable_type                          6000 non-null   object        \n",
      " 2   started_at                             6000 non-null   datetime64[ns]\n",
      " 3   ended_at                               6000 non-null   datetime64[ns]\n",
      " 4   start_station_name                     6000 non-null   object        \n",
      " 5   start_station_id                       6000 non-null   object        \n",
      " 6   end_station_name                       6000 non-null   object        \n",
      " 7   end_station_id                         6000 non-null   object        \n",
      " 8   start_lat                              6000 non-null   float64       \n",
      " 9   start_lng                              6000 non-null   float64       \n",
      " 10  end_lat                                6000 non-null   float64       \n",
      " 11  end_lng                                6000 non-null   float64       \n",
      " 12  member_casual                          6000 non-null   object        \n",
      " 13  Unnamed: 0                             500 non-null    float64       \n",
      " 14  rideable_type_duplicate_column_name_1  500 non-null    object        \n",
      " 15  started_at_date                        6000 non-null   datetime64[ns]\n",
      " 16  started_at_time                        6000 non-null   object        \n",
      " 17  ended_at_date                          6000 non-null   datetime64[ns]\n",
      " 18  ended_at_time                          6000 non-null   object        \n",
      "dtypes: datetime64[ns](4), float64(5), object(10)\n",
      "memory usage: 890.8+ KB\n"
     ]
    }
   ],
   "source": [
    "tripdata_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped columns: 'Unnamed: 0' and 'rideable_type_duplicate_column_name_1'.\n",
      "\n",
      "DataFrame has been successfully saved to 'citibike_tripdata.csv'.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   ride_id             6000 non-null   object        \n",
      " 1   rideable_type       6000 non-null   object        \n",
      " 2   started_at          6000 non-null   datetime64[ns]\n",
      " 3   ended_at            6000 non-null   datetime64[ns]\n",
      " 4   start_station_name  6000 non-null   object        \n",
      " 5   start_station_id    6000 non-null   object        \n",
      " 6   end_station_name    6000 non-null   object        \n",
      " 7   end_station_id      6000 non-null   object        \n",
      " 8   start_lat           6000 non-null   float64       \n",
      " 9   start_lng           6000 non-null   float64       \n",
      " 10  end_lat             6000 non-null   float64       \n",
      " 11  end_lng             6000 non-null   float64       \n",
      " 12  member_casual       6000 non-null   object        \n",
      " 13  started_at_date     6000 non-null   datetime64[ns]\n",
      " 14  started_at_time     6000 non-null   object        \n",
      " 15  ended_at_date       6000 non-null   datetime64[ns]\n",
      " 16  ended_at_time       6000 non-null   object        \n",
      "dtypes: datetime64[ns](4), float64(4), object(9)\n",
      "memory usage: 797.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Drop Unwanted Columns --------------------\n",
    "\n",
    "# 1. Drop the 'Unnamed' and 'rideable_type_duplicate_column_name_1' columns if they exist\n",
    "# Use errors='ignore' to avoid errors if the columns don't exist in the DataFrame\n",
    "tripdata_df.drop(columns=['Unnamed: 0', 'rideable_type_duplicate_column_name_1'], errors='ignore', inplace=True)\n",
    "\n",
    "# Print a confirmation message that the columns have been dropped\n",
    "print(\"\\nDropped columns: 'Unnamed: 0' and 'rideable_type_duplicate_column_name_1'.\")\n",
    "\n",
    "# -------------------- Save the Cleaned DataFrame to a CSV File --------------------\n",
    "\n",
    "# 2. Save the cleaned DataFrame to a CSV file named 'citibike_tripdata.csv'\n",
    "# index=False prevents pandas from writing the row index to the CSV file\n",
    "tripdata_df.to_csv('citibike_tripdata.csv', index=False)\n",
    "\n",
    "# Print a confirmation message indicating that the DataFrame has been saved\n",
    "print(\"\\nDataFrame has been successfully saved to 'citibike_tripdata.csv'.\")\n",
    "\n",
    "tripdata_df.info()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
